{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456fdace-1824-442c-a480-79270ae035b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "import sklearn.metrics as m\n",
    "import boost_histogram as bh\n",
    "import glob\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "from cycler import cycler\n",
    "import mplhep as hep\n",
    "# plt.style.use(hep.style.ROOT)\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "def _p4_from_ptetaphie(pt, eta, phi, energy):\n",
    "    import vector\n",
    "    vector.register_awkward()\n",
    "    return vector.zip({'pt': pt, 'eta': eta, 'phi': phi, 'energy': energy})\n",
    "def _p4_from_ptetaphim(pt, eta, phi, mass):\n",
    "    import vector\n",
    "    vector.register_awkward()\n",
    "    return vector.zip({'pt': pt, 'eta': eta, 'phi': phi, 'mass': mass})\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import re\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from scipy.interpolate import interp1d, RectBivariateSpline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1bd645-a6aa-464b-8aa8-11a25e7632f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Loading Universal Correctors for all models...\n",
      "================================================================================\n",
      "\n",
      "Initializing corrector for Ensemble...\n",
      "Loading source distributions for Ensemble...\n",
      "Found 9 score bins to process.\n",
      "\n",
      "  [Corrector] Processing bin [0.000, 0.100)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9888\n",
      "  [Corrector] Processing bin [0.100, 0.300)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9933\n",
      "  [Corrector] Processing bin [0.300, 0.500)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9942\n",
      "  [Corrector] Processing bin [0.500, 0.700)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9948\n",
      "  [Corrector] Processing bin [0.700, 0.900)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9956\n",
      "  [Corrector] Processing bin [0.900, 0.950)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9961\n",
      "  [Corrector] Processing bin [0.950, 0.990)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9969\n",
      "  [Corrector] Processing bin [0.990, 0.997)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9975\n",
      "  [Corrector] Processing bin [0.997, 1.000)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9945\n",
      "  Success: Ensemble corrector loaded\n",
      "\n",
      "Initializing corrector for Model0...\n",
      "Loading source distributions for Model0...\n",
      "Found 9 score bins to process.\n",
      "\n",
      "  [Corrector] Processing bin [0.000, 0.100)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9888\n",
      "  [Corrector] Processing bin [0.100, 0.300)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9933\n",
      "  [Corrector] Processing bin [0.300, 0.500)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9942\n",
      "  [Corrector] Processing bin [0.500, 0.700)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9948\n",
      "  [Corrector] Processing bin [0.700, 0.900)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9956\n",
      "  [Corrector] Processing bin [0.900, 0.950)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9961\n",
      "  [Corrector] Processing bin [0.950, 0.990)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9969\n",
      "  [Corrector] Processing bin [0.990, 0.997)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9975\n",
      "  [Corrector] Processing bin [0.997, 1.000)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9945\n",
      "  Success: Model0 corrector loaded\n",
      "\n",
      "Initializing corrector for Model1...\n",
      "Loading source distributions for Model1...\n",
      "Found 9 score bins to process.\n",
      "\n",
      "  [Corrector] Processing bin [0.000, 0.100)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9888\n",
      "  [Corrector] Processing bin [0.100, 0.300)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9933\n",
      "  [Corrector] Processing bin [0.300, 0.500)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9942\n",
      "  [Corrector] Processing bin [0.500, 0.700)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9948\n",
      "  [Corrector] Processing bin [0.700, 0.900)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9956\n",
      "  [Corrector] Processing bin [0.900, 0.950)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9961\n",
      "  [Corrector] Processing bin [0.950, 0.990)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9969\n",
      "  [Corrector] Processing bin [0.990, 0.997)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9975\n",
      "  [Corrector] Processing bin [0.997, 1.000)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9945\n",
      "  Success: Model1 corrector loaded\n",
      "\n",
      "Initializing corrector for Model2...\n",
      "Loading source distributions for Model2...\n",
      "Found 9 score bins to process.\n",
      "\n",
      "  [Corrector] Processing bin [0.000, 0.100)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9888\n",
      "  [Corrector] Processing bin [0.100, 0.300)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9933\n",
      "  [Corrector] Processing bin [0.300, 0.500)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9942\n",
      "  [Corrector] Processing bin [0.500, 0.700)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9948\n",
      "  [Corrector] Processing bin [0.700, 0.900)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9956\n",
      "  [Corrector] Processing bin [0.900, 0.950)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9961\n",
      "  [Corrector] Processing bin [0.950, 0.990)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9969\n",
      "  [Corrector] Processing bin [0.990, 0.997)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9975\n",
      "  [Corrector] Processing bin [0.997, 1.000)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9945\n",
      "  Success: Model2 corrector loaded\n",
      "\n",
      "Initializing corrector for ModelLite...\n",
      "Loading source distributions for ModelLite...\n",
      "Found 9 score bins to process.\n",
      "\n",
      "  [Corrector] Processing bin [0.000, 0.100)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9888\n",
      "  [Corrector] Processing bin [0.100, 0.300)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9933\n",
      "  [Corrector] Processing bin [0.300, 0.500)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9942\n",
      "  [Corrector] Processing bin [0.500, 0.700)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9948\n",
      "  [Corrector] Processing bin [0.700, 0.900)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9956\n",
      "  [Corrector] Processing bin [0.900, 0.950)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9961\n",
      "  [Corrector] Processing bin [0.950, 0.990)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9969\n",
      "  [Corrector] Processing bin [0.990, 0.997)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9975\n",
      "  [Corrector] Processing bin [0.997, 1.000)...\n",
      "  Source interpolator built: range [50.0, 190.0] GeV\n",
      "  Target polynomial fit R^2 = 0.9945\n",
      "  Success: ModelLite corrector loaded\n",
      "\n",
      "================================================================================\n",
      "Corrector initialization complete\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Starting Ensemble Method2 Ntuple Generation\n",
      "with Diagonal Slice CDF Correction\n",
      "================================================================================\n",
      "\n",
      "Output directory: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2\n",
      "Available CPU cores: 72\n",
      "Using 40 workers\n",
      "\n",
      "Correction method: Diagonal slice-based CDF matching with dynamic extension\n",
      "  - Source: Saved smooth 2D distributions\n",
      "  - Target: Tomography-corrected distributions\n",
      "  - Correction range (M): [70, 170] GeV\n",
      "  - Extended sampling range: [60, 180] GeV (linear from center)\n",
      "    * M = 120: sampling in [70, 170]\n",
      "    * M = 70:  sampling in [60, 170]\n",
      "    * M = 170: sampling in [70, 180]\n",
      "\n",
      "Modellite processing:\n",
      "  - Enabled for: ggHH, QCD, TTbar, SingleTop, TW, TTbarW, TTbarZ, WW, ZW, ZZ, SingleHiggs, VBFH, WplusH, WminusH, ZH, ttH, ZJetsToQQ\n",
      "================================================================================\n",
      "\n",
      "Collecting file list...\n",
      "Found 16 files to process\n",
      "Using 40 parallel workers\n",
      "[Worker 3287229] [7/16] Processing: ggHHkl5_id400-599[Worker 3287232] [10/16] Processing: ggHHkl5_id600-799[Worker 3287223] [1/16] Processing: qqHHCV1C2V2kl1_merged[Worker 3287224] [2/16] Processing: qqHHCV1C2V1kl2_merged[Worker 3287227] [5/16] Processing: qqHHCV1p5C2V1kl1_merged[Worker 3287226] [4/16] Processing: qqHH_merged[Worker 3287236] [14/16] Processing: ggHHkl0_id800-999[Worker 3287225] [3/16] Processing: qqHHCV0p5C2V1kl1_merged[Worker 3287230] [8/16] Processing: ggHHkl5_id800-999[Worker 3287228] [6/16] Processing: qqHHCV1C2V1kl0_merged[Worker 3287231] [9/16] Processing: ggHHkl5_id0-199[Worker 3287233] [11/16] Processing: ggHHkl5_id200-399\n",
      "[Worker 3287235] [13/16] Processing: ggHHkl0_id400-599\n",
      "[Worker 3287234] [12/16] Processing: ggHHkl0_id0-199\n",
      "\n",
      "\n",
      "\n",
      "[Worker 3287237] [15/16] Processing: ggHHkl0_id200-399[Worker 3287238] [16/16] Processing: ggHHkl0_id600-799\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Worker 3287232] Saved 156977 events (151999 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/ggHHkl5/ensemble_method2_ggHHkl5_id600-799.root (without modellite)\n",
      "[Worker 3287233] Saved 157048 events (152095 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/ggHHkl5/ensemble_method2_ggHHkl5_id200-399.root (without modellite)\n",
      "[Worker 3287229] Saved 157185 events (152106 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/ggHHkl5/ensemble_method2_ggHHkl5_id400-599.root (without modellite)\n",
      "[Worker 3287231] Saved 156750 events (151683 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/ggHHkl5/ensemble_method2_ggHHkl5_id0-199.root (without modellite)\n",
      "[Worker 3287230] Saved 156960 events (152116 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/ggHHkl5/ensemble_method2_ggHHkl5_id800-999.root (without modellite)\n",
      "[Worker 3287226] Saved 303817 events (292100 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/qqHH/ensemble_method2_qqHH_merged.root (without modellite)\n",
      "[Worker 3287228] Saved 327300 events (315149 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/qqHHCV1C2V1kl0/ensemble_method2_qqHHCV1C2V1kl0_merged.root (without modellite)\n",
      "[Worker 3287236] Saved 324741 events (313942 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/ggHHkl0/ensemble_method2_ggHHkl0_id800-999.root (without modellite)\n",
      "[Worker 3287235] Saved 324145 events (313174 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/ggHHkl0/ensemble_method2_ggHHkl0_id400-599.root (without modellite)\n",
      "[Worker 3287234] Saved 322701 events (312022 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/ggHHkl0/ensemble_method2_ggHHkl0_id0-199.root (without modellite)\n",
      "[Worker 3287238] Saved 323830 events (313026 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/ggHHkl0/ensemble_method2_ggHHkl0_id600-799.root (without modellite)\n",
      "[Worker 3287237] Saved 324658 events (314021 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/ggHHkl0/ensemble_method2_ggHHkl0_id200-399.root (without modellite)\n",
      "[Worker 3287224] Saved 390998 events (376546 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/qqHHCV1C2V1kl2/ensemble_method2_qqHHCV1C2V1kl2_merged.root (without modellite)\n",
      "[Worker 3287227] Saved 697678 events (671696 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/qqHHCV1p5C2V1kl1/ensemble_method2_qqHHCV1p5C2V1kl1_merged.root (without modellite)\n",
      "[Worker 3287225] Saved 987705 events (951681 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/qqHHCV0p5C2V1kl1/ensemble_method2_qqHHCV0p5C2V1kl1_merged.root (without modellite)\n",
      "[Worker 3287223] Saved 1104735 events (1064189 pass combined_basic_fit_cut) to: /data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2/qqHHCV1C2V2kl1/ensemble_method2_qqHHCV1C2V2kl1_merged.root (without modellite)\n",
      "\n",
      "================================================================================\n",
      "Processing Summary\n",
      "================================================================================\n",
      "Total files processed: 16\n",
      "Successful: 16\n",
      "Failed: 0\n",
      "\n",
      "Total events saved: 6217228\n",
      "Events passing combined_basic_fit_cut: 5997545 (96.47%)\n",
      "Files with modellite: 0\n",
      "Files without modellite: 16\n",
      "\n",
      "Summary by process:\n",
      "  ggHHkl0 [modellite:✗]: 5 files, 1620075 events, 1566185 pass combined cut (96.67%)\n",
      "  ggHHkl5 [modellite:✗]: 5 files, 784920 events, 759999 pass combined cut (96.83%)\n",
      "  qqHH [modellite:✗]: 1 files, 303817 events, 292100 pass combined cut (96.14%)\n",
      "  qqHHCV0p5C2V1kl1 [modellite:✗]: 1 files, 987705 events, 951681 pass combined cut (96.35%)\n",
      "  qqHHCV1C2V1kl0 [modellite:✗]: 1 files, 327300 events, 315149 pass combined cut (96.29%)\n",
      "  qqHHCV1C2V1kl2 [modellite:✗]: 1 files, 390998 events, 376546 pass combined cut (96.30%)\n",
      "  qqHHCV1C2V2kl1 [modellite:✗]: 1 files, 1104735 events, 1064189 pass combined cut (96.33%)\n",
      "  qqHHCV1p5C2V1kl1 [modellite:✗]: 1 files, 697678 events, 671696 pass combined cut (96.28%)\n",
      "\n",
      "================================================================================\n",
      "Processing completed!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "lumi_scale = 4.5  # 450 fb^-1\n",
    "\n",
    "# Weight dictionary\n",
    "weight_dict = {\n",
    "    \"QCD\": lumi_scale * 4.5226e+06 * 1e5 / ((17600+38072) * 50e5),\n",
    "    \"ZJetsToQQ\": lumi_scale * 1253.61 * 1e5 / (4000 * 1e5),\n",
    "    \"TTbar\": lumi_scale * 83175900 / (40305472 + 120928855),\n",
    "    \"SingleTop\": lumi_scale * 18487900 / 17032212,\n",
    "    \"TW\": lumi_scale * 6496000 / 4694318,\n",
    "    \"TTbarW\": lumi_scale * 74530 / 1000000,\n",
    "    \"TTbarZ\": lumi_scale * 85900 / 1000000,\n",
    "    \"WW\": lumi_scale * 11870000 / 14330905,\n",
    "    \"ZW\": lumi_scale * 4674000 / 7117197,\n",
    "    \"ZZ\": lumi_scale * 1691000 / 7055884,\n",
    "    \"ZZherwig\": lumi_scale * 1691000 / 9899325,\n",
    "    \"ZZvincia\": lumi_scale * 1691000 / 10000000,\n",
    "    \"SingleHiggs\": lumi_scale * 4858000 / 10000000,\n",
    "    \"VBFH\": lumi_scale * 378200 / 1000000,\n",
    "    \"WplusH\": lumi_scale * 83990 / 499991,\n",
    "    \"WminusH\": lumi_scale * 53270 / 499999,\n",
    "    \"ZH\": lumi_scale * 76120 / 300000,\n",
    "    \"ttH\": lumi_scale * 50710 / 300000,\n",
    "    \"ggHH\": lumi_scale * 1051.7 / 10000000,\n",
    "    \"ggHHherwig\": lumi_scale * 1051.7 / 10000000,\n",
    "    \"ggHHvincia\": lumi_scale * 1051.7 / 10000000,\n",
    "    \"ggHHkl0\": lumi_scale * 2361.8 / 10000000,\n",
    "    \"ggHHkl5\": lumi_scale * 3107.2 / 10000000,\n",
    "    \"qqHH\": lumi_scale * 58.5 / 3000000,\n",
    "    \"qqHHCV1C2V1kl0\": lumi_scale * 155.8 / 3000000,\n",
    "    \"qqHHCV1C2V1kl2\": lumi_scale * 48.2 / 3000000,\n",
    "    \"qqHHCV1C2V2kl1\": lumi_scale * 482.3 / 3000000,\n",
    "    \"qqHHCV0p5C2V1kl1\": lumi_scale * 365.6 / 3000000,\n",
    "    \"qqHHCV1p5C2V1kl1\": lumi_scale * 2241.2 / 3000000,\n",
    "}\n",
    "\n",
    "# Process list\n",
    "# process_list = ['ggHH', 'QCD', 'TTbar', 'SingleTop', 'TW', 'TTbarW', 'TTbarZ', \n",
    "#                 'WW', 'ZW', 'ZZ', 'SingleHiggs', 'VBFH', 'WplusH', 'WminusH', \n",
    "#                 'ZH', 'ttH', 'ZJetsToQQ']\n",
    "\n",
    "process_list = ['ggHHkl0', 'ggHHkl5', \"qqHH\",\n",
    "    \"qqHHCV1C2V1kl0\",\n",
    "    \"qqHHCV1C2V1kl2\",\n",
    "    \"qqHHCV1C2V2kl1\",\n",
    "    \"qqHHCV0p5C2V1kl1\",\n",
    "    \"qqHHCV1p5C2V1kl1\",]\n",
    "\n",
    "PROCESS_WITH_MODELLITE = ['ggHH', 'QCD', 'TTbar', 'SingleTop', 'TW', 'TTbarW', 'TTbarZ', \n",
    "                          'WW', 'ZW', 'ZZ', 'SingleHiggs', 'VBFH', 'WplusH', 'WminusH', \n",
    "                          'ZH', 'ttH', 'ZJetsToQQ']\n",
    "\n",
    "ensemble_models = [\n",
    "    \"../../predict/hh4b_resolved_newsp4_allparts_nosel_138clswtop.noweights.ddp4-bs512-lr2e-3\",\n",
    "    \"../../predict/hh4b_resolved_newsp4_allparts_nosel_138clswtop.noweights.ddp4-bs512-lr2e-3.model1\", \n",
    "    \"../../predict/hh4b_resolved_newsp4_allparts_nosel_138clswtop.noweights.ddp4-bs512-lr2e-3.model2\",\n",
    "    \"../../predict/hh4b_resolved_newsp4_allparts_nosel_138clswtop.noweights.ndiv10.ddp4-bs400-lr1p6e-3.wd0p01\"\n",
    "]\n",
    "\n",
    "pred_folder = \"/home/olympus/tyyang99/weaver-core-dev/weaver/pheno/predict\"\n",
    "folder_pattern = \"/data/bond/tyyang99/HH4b/sm_incl_derived_4j3bor2b/*\"\n",
    "\n",
    "output_base_dir = \"/data/bond/tyyang99/HH4b/ensemble_method2_ntuples_cdfv2\"\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "N_WORKERS = 40\n",
    "\n",
    "# ===============================================================================\n",
    "# NEW: Load Universal Corrector\n",
    "# ===============================================================================\n",
    "\n",
    "from scipy.interpolate import interp1d, RectBivariateSpline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# ==============================================================================\n",
    "# Corrector Classes\n",
    "# ==============================================================================\n",
    "\n",
    "class SavedSmoothSurfaceInterpolator:\n",
    "    def __init__(self):\n",
    "        self.interpolator = None\n",
    "        self.fitted = False\n",
    "        self.m_min = None\n",
    "        self.m_max = None\n",
    "\n",
    "    def fit(self, H_smooth, bins):\n",
    "        H_sym = (H_smooth + H_smooth.T) / 2.0\n",
    "        centers = (bins[:-1] + bins[1:]) / 2\n",
    "        \n",
    "        self.interpolator = RectBivariateSpline(\n",
    "            centers, centers, H_sym, \n",
    "            kx=3, ky=3\n",
    "        )\n",
    "        \n",
    "        self.m_min = bins[0]\n",
    "        self.m_max = bins[-1]\n",
    "        self.fitted = True\n",
    "        \n",
    "        print(f\"  Source interpolator built: range [{self.m_min:.1f}, {self.m_max:.1f}] GeV\")\n",
    "        return True\n",
    "    \n",
    "    def evaluate(self, m1, m2):\n",
    "        if not self.fitted:\n",
    "            return np.zeros_like(m1)\n",
    "        \n",
    "        m1_flat = np.atleast_1d(m1).flatten()\n",
    "        m2_flat = np.atleast_1d(m2).flatten()\n",
    "        \n",
    "        m1_clipped = np.clip(m1_flat, self.m_min, self.m_max)\n",
    "        m2_clipped = np.clip(m2_flat, self.m_min, self.m_max)\n",
    "        \n",
    "        z_pred = self.interpolator.ev(m1_clipped, m2_clipped)\n",
    "        z_pred = np.maximum(z_pred, 0)\n",
    "        \n",
    "        if np.isscalar(m1):\n",
    "            return float(z_pred[0])\n",
    "        else:\n",
    "            return z_pred.reshape(np.shape(m1))\n",
    "\n",
    "class SmoothSurfaceFitter:\n",
    "    def __init__(self, degree=4):\n",
    "        self.degree = degree\n",
    "        self.model = LinearRegression()\n",
    "        self.poly = PolynomialFeatures(degree=degree)\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, H_coarse, bins_coarse):\n",
    "        H_sym = (H_coarse + H_coarse.T) / 2.0\n",
    "        \n",
    "        c_coarse = (bins_coarse[:-1] + bins_coarse[1:]) / 2\n",
    "        X_grid, Y_grid = np.meshgrid(c_coarse, c_coarse)\n",
    "        x_flat, y_flat = X_grid.flatten(), Y_grid.flatten()\n",
    "        z_flat = H_sym.flatten()\n",
    "        \n",
    "        mask = z_flat > 0\n",
    "        if np.sum(mask) < 10:\n",
    "            print(\"  [Warning] Not enough data points for fitting!\")\n",
    "            return False\n",
    "            \n",
    "        X_train = np.column_stack((x_flat[mask], y_flat[mask]))\n",
    "        z_log = np.log(z_flat[mask])\n",
    "        \n",
    "        X_poly = self.poly.fit_transform(X_train)\n",
    "        self.model.fit(X_poly, z_log)\n",
    "        \n",
    "        score = self.model.score(X_poly, z_log)\n",
    "        print(f\"  Target polynomial fit R^2 = {score:.4f}\")\n",
    "        \n",
    "        self.fitted = True\n",
    "        return True\n",
    "    \n",
    "    def evaluate(self, m1, m2):\n",
    "        if not self.fitted:\n",
    "            return np.zeros_like(m1)\n",
    "        \n",
    "        m1_flat = np.atleast_1d(m1).flatten()\n",
    "        m2_flat = np.atleast_1d(m2).flatten()\n",
    "        \n",
    "        X_test = np.column_stack((m1_flat, m2_flat))\n",
    "        X_test_poly = self.poly.transform(X_test)\n",
    "        z_pred_log = self.model.predict(X_test_poly)\n",
    "        z_pred = np.exp(z_pred_log)\n",
    "        \n",
    "        if np.isscalar(m1):\n",
    "            return float(z_pred[0])\n",
    "        else:\n",
    "            return z_pred.reshape(np.shape(m1))\n",
    "\n",
    "class ContinuousSliceMatcher:\n",
    "    def __init__(self, M_value, m_min_correct=70, m_max_correct=170, \n",
    "                 m_min_extended=60, m_max_extended=180, n_samples=1000):\n",
    "        self.M = M_value\n",
    "        self.m_min_correct = m_min_correct\n",
    "        self.m_max_correct = m_max_correct\n",
    "        self.m_min_extended = m_min_extended\n",
    "        self.m_max_extended = m_max_extended\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "        self.M_center = (m_min_correct + m_max_correct) / 2\n",
    "        \n",
    "        self.cdf_source = None\n",
    "        self.inv_cdf_target = None\n",
    "    \n",
    "    def _get_sampling_bounds(self):\n",
    "        dist_to_center = abs(self.M - self.M_center)\n",
    "        max_dist_to_center = self.M_center - self.m_min_correct\n",
    "        \n",
    "        normalized_dist = min(dist_to_center / max_dist_to_center, 1.0)\n",
    "        \n",
    "        if self.M < self.M_center:\n",
    "            extension_lower = (self.m_min_correct - self.m_min_extended) * normalized_dist\n",
    "            extension_upper = 0\n",
    "        else:\n",
    "            extension_lower = 0\n",
    "            extension_upper = (self.m_max_extended - self.m_max_correct) * normalized_dist\n",
    "        \n",
    "        m_min_use = self.m_min_correct - extension_lower\n",
    "        m_max_use = self.m_max_correct + extension_upper\n",
    "        \n",
    "        return m_min_use, m_max_use\n",
    "    \n",
    "    def fit(self, source_func, target_func):\n",
    "        m_min_use, m_max_use = self._get_sampling_bounds()\n",
    "        \n",
    "        self.Delta_max_sampling = 2 * min(self.M - m_min_use, m_max_use - self.M)\n",
    "        \n",
    "        if self.Delta_max_sampling <= 1e-6:\n",
    "            return False\n",
    "        \n",
    "        self.m_min_sampling = m_min_use\n",
    "        self.m_max_sampling = m_max_use\n",
    "        \n",
    "        rho_samples = np.linspace(0, 1, self.n_samples)\n",
    "        Delta_samples = rho_samples * self.Delta_max_sampling\n",
    "        \n",
    "        m1_samples = self.M - Delta_samples / 2\n",
    "        m2_samples = self.M + Delta_samples / 2\n",
    "        \n",
    "        mask_valid = (m1_samples >= m_min_use) & (m1_samples <= m_max_use) & \\\n",
    "                     (m2_samples >= m_min_use) & (m2_samples <= m_max_use)\n",
    "        \n",
    "        if np.sum(mask_valid) < 10:\n",
    "            return False\n",
    "        \n",
    "        m1_valid = m1_samples[mask_valid]\n",
    "        m2_valid = m2_samples[mask_valid]\n",
    "        rho_valid = rho_samples[mask_valid]\n",
    "        \n",
    "        density_source = source_func(m1_valid, m2_valid)\n",
    "        density_target = target_func(m1_valid, m2_valid)\n",
    "        \n",
    "        pdf_source = density_source * self.Delta_max_sampling\n",
    "        pdf_target = density_target * self.Delta_max_sampling\n",
    "        \n",
    "        pdf_source = pdf_source / (np.sum(pdf_source) + 1e-9)\n",
    "        pdf_target = pdf_target / (np.sum(pdf_target) + 1e-9)\n",
    "        \n",
    "        cdf_source = np.cumsum(pdf_source)\n",
    "        cdf_source = cdf_source / (cdf_source[-1] + 1e-9)\n",
    "        \n",
    "        cdf_target = np.cumsum(pdf_target)\n",
    "        cdf_target = cdf_target / (cdf_target[-1] + 1e-9)\n",
    "        \n",
    "        rho_min = rho_valid[0]\n",
    "        rho_max = rho_valid[-1]\n",
    "        \n",
    "        self.cdf_source = interp1d(\n",
    "            rho_valid, cdf_source,\n",
    "            kind='linear', bounds_error=False, fill_value=(0, 1)\n",
    "        )\n",
    "        \n",
    "        self.inv_cdf_target = interp1d(\n",
    "            cdf_target, rho_valid,\n",
    "            kind='linear', bounds_error=False, fill_value=(rho_min, rho_max)\n",
    "        )\n",
    "        \n",
    "        self.rho_min = rho_min\n",
    "        self.rho_max = rho_max\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def transform(self, m1, m2):\n",
    "        if self.cdf_source is None or self.inv_cdf_target is None:\n",
    "            return m1, m2\n",
    "        \n",
    "        M = (m1 + m2) / 2\n",
    "        Delta = np.abs(m1 - m2)\n",
    "        \n",
    "        rho = Delta / self.Delta_max_sampling\n",
    "        rho = np.clip(rho, self.rho_min, self.rho_max)\n",
    "        \n",
    "        u = self.cdf_source(rho)\n",
    "        u = np.clip(u, 0, 1)\n",
    "        rho_new = self.inv_cdf_target(u)\n",
    "        \n",
    "        Delta_new = rho_new * self.Delta_max_sampling\n",
    "        m1_new = M - Delta_new / 2\n",
    "        m2_new = M + Delta_new / 2\n",
    "        \n",
    "        return m1_new, m2_new\n",
    "\n",
    "\n",
    "class ContinuousDiagonalCorrector:\n",
    "    def __init__(self, m_min_fit=50, m_max_fit=190, \n",
    "                 m_min_correct=70, m_max_correct=170,\n",
    "                 m_min_extended=60, m_max_extended=180,\n",
    "                 n_slices=40):\n",
    "        self.m_min_fit = m_min_fit\n",
    "        self.m_max_fit = m_max_fit\n",
    "        self.m_min_correct = m_min_correct\n",
    "        self.m_max_correct = m_max_correct\n",
    "        self.m_min_extended = m_min_extended\n",
    "        self.m_max_extended = m_max_extended\n",
    "        \n",
    "        self.M_values = np.linspace(m_min_correct, m_max_correct, n_slices)\n",
    "        \n",
    "        self.source_func = None\n",
    "        self.target_func = None\n",
    "        \n",
    "        self.matchers = []\n",
    "    \n",
    "    def fit_from_data(self, H_source_smooth, bins_source, H_target, bins_target):\n",
    "        interp_source = SavedSmoothSurfaceInterpolator()\n",
    "        success_src = interp_source.fit(H_source_smooth, bins_source)\n",
    "        \n",
    "        fitter_target = SmoothSurfaceFitter(degree=4)\n",
    "        success_tgt = fitter_target.fit(H_target, bins_target)\n",
    "        \n",
    "        if not (success_src and success_tgt):\n",
    "            print(\"  [Error] Failed to build functions!\")\n",
    "            return\n",
    "        \n",
    "        self.source_func = interp_source.evaluate\n",
    "        self.target_func = fitter_target.evaluate\n",
    "        \n",
    "        for M_val in self.M_values:\n",
    "            matcher = ContinuousSliceMatcher(\n",
    "                M_val, \n",
    "                self.m_min_correct, self.m_max_correct,\n",
    "                self.m_min_extended, self.m_max_extended,\n",
    "                n_samples=1000\n",
    "            )\n",
    "            \n",
    "            success = matcher.fit(self.source_func, self.target_func)\n",
    "            \n",
    "            if success:\n",
    "                self.matchers.append((M_val, matcher))\n",
    "\n",
    "    \n",
    "    def correct(self, m1, m2):\n",
    "        if len(self.matchers) == 0:\n",
    "            print(\"  [Warning] No matchers available!\")\n",
    "            return m1, m2\n",
    "        \n",
    "        M = (m1 + m2) / 2\n",
    "        m1_new = np.copy(m1)\n",
    "        m2_new = np.copy(m2)\n",
    "        \n",
    "        M_slice_values = np.array([m for m, _ in self.matchers])\n",
    "        \n",
    "        mask_M_in_range = (M >= self.m_min_correct) & (M <= self.m_max_correct)\n",
    "        \n",
    "        for i in np.where(mask_M_in_range)[0]:\n",
    "            idx = np.argmin(np.abs(M_slice_values - M[i]))\n",
    "            M_slice, matcher = self.matchers[idx]\n",
    "            \n",
    "            m_min_slice = matcher.m_min_sampling\n",
    "            m_max_slice = matcher.m_max_sampling\n",
    "            \n",
    "            if (m1[i] >= m_min_slice and m1[i] <= m_max_slice and\n",
    "                m2[i] >= m_min_slice and m2[i] <= m_max_slice):\n",
    "                m1_new[i], m2_new[i] = matcher.transform(m1[i], m2[i])\n",
    "        \n",
    "        return m1_new, m2_new\n",
    "\n",
    "\n",
    "class UniversalCorrector:\n",
    "    def __init__(self, source_dir, model_name, tomography_dir, \n",
    "                 m_min_fit=50, m_max_fit=190, \n",
    "                 m_min_correct=70, m_max_correct=170,\n",
    "                 m_min_extended=60, m_max_extended=180):\n",
    "        self.m_min_fit = m_min_fit\n",
    "        self.m_max_fit = m_max_fit\n",
    "        self.m_min_correct = m_min_correct\n",
    "        self.m_max_correct = m_max_correct\n",
    "        self.m_min_extended = m_min_extended\n",
    "        self.m_max_extended = m_max_extended\n",
    "\n",
    "        \n",
    "        self.correctors = {}\n",
    "        \n",
    "        source_npz = os.path.join(source_dir, f\"source_distributions_{model_name}.npz\")\n",
    "        \n",
    "        print(f\"Loading source distributions for {model_name}...\")\n",
    "        src_data = np.load(source_npz)\n",
    "        src_bins = src_data['bins']\n",
    "        \n",
    "        saved_model_name = str(src_data['model_name'])\n",
    "        if saved_model_name != model_name:\n",
    "            print(f\"  [Warning] Model name mismatch: expected {model_name}, got {saved_model_name}\")\n",
    "        \n",
    "        keys = [k for k in src_data.keys() if \"_smooth\" in k]\n",
    "        \n",
    "        print(f\"Found {len(keys)} score bins to process.\\n\")\n",
    "        \n",
    "        for k in keys:\n",
    "            parts = k.replace(\"_smooth\", \"\").split(\"_\")\n",
    "            low = float(parts[1].replace(\"p\", \".\"))\n",
    "            high = float(parts[2].replace(\"p\", \".\"))\n",
    "            \n",
    "            H_src_smooth = src_data[k]\n",
    "            \n",
    "            tgt_folder = f\"range_{parts[1]}_{parts[2]}\"\n",
    "            tgt_path = f\"{tomography_dir}/{tgt_folder}/data.npz\"\n",
    "            \n",
    "            if not os.path.exists(tgt_path):\n",
    "                print(f\"  [Warning] Target not found for bin [{low}, {high})\")\n",
    "                continue\n",
    "            \n",
    "            tgt_data = np.load(tgt_path)\n",
    "            H_tgt = tgt_data['corrected']\n",
    "            tgt_bins = tgt_data['bins']\n",
    "            \n",
    "            print(f\"  [Corrector] Processing bin [{low:.3f}, {high:.3f})...\")\n",
    "            \n",
    "            corrector = ContinuousDiagonalCorrector(\n",
    "                m_min_fit=m_min_fit, m_max_fit=m_max_fit,\n",
    "                m_min_correct=m_min_correct, m_max_correct=m_max_correct,\n",
    "                m_min_extended=self.m_min_extended, m_max_extended=self.m_max_extended,\n",
    "                n_slices=40\n",
    "            )\n",
    "\n",
    "            \n",
    "            corrector.fit_from_data(H_src_smooth, src_bins, H_tgt, tgt_bins)\n",
    "            \n",
    "            self.correctors[(low, high)] = corrector\n",
    "    \n",
    "    def correct(self, m1, m2, scores):\n",
    "        m1_out = np.copy(m1)\n",
    "        m2_out = np.copy(m2)\n",
    "        \n",
    "        for (low, high), corrector in self.correctors.items():\n",
    "            if high == 1.0:\n",
    "                mask_score = (scores >= low) & (scores <= high)\n",
    "            else:\n",
    "                mask_score = (scores >= low) & (scores < high)\n",
    "            \n",
    "            if np.sum(mask_score) == 0:\n",
    "                continue\n",
    "            \n",
    "            m1_sub = m1[mask_score]\n",
    "            m2_sub = m2[mask_score]\n",
    "            \n",
    "            m1_corr, m2_corr = corrector.correct(m1_sub, m2_sub)\n",
    "            \n",
    "            m1_out[mask_score] = m1_corr\n",
    "            m2_out[mask_score] = m2_corr\n",
    "        \n",
    "        return m1_out, m2_out\n",
    "\n",
    "\n",
    "SOURCE_DIR = \"./source_2d_distributions\"\n",
    "TOMOGRAPHY_DIR = \"./tomography_output\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Loading Universal Correctors for all models...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "CORRECTORS = {}\n",
    "MODEL_NAMES = [\"Ensemble\", \"Model0\", \"Model1\", \"Model2\", \"ModelLite\"]\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    print(f\"\\nInitializing corrector for {model_name}...\")\n",
    "    try:\n",
    "        corrector = UniversalCorrector(\n",
    "            SOURCE_DIR, model_name, TOMOGRAPHY_DIR,\n",
    "            m_min_fit=50, m_max_fit=190,\n",
    "            m_min_correct=70, m_max_correct=170,\n",
    "            m_min_extended=60, m_max_extended=180\n",
    "        )\n",
    "\n",
    "        CORRECTORS[model_name] = corrector\n",
    "        print(f\"  Success: {model_name} corrector loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading {model_name} corrector: {e}\")\n",
    "        CORRECTORS[model_name] = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Corrector initialization complete\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ===============================================================================\n",
    "# Helper Functions\n",
    "# ===============================================================================\n",
    "\n",
    "def apply_correction(original_x, original_y, scores, corrector):\n",
    "    \"\"\"\n",
    "    Apply correction using UniversalCorrector\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_x, original_y : array-like\n",
    "        Original peak positions\n",
    "    scores : array-like\n",
    "        Model scores\n",
    "    corrector : UniversalCorrector\n",
    "        Corrector instance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    corrected_x, corrected_y : arrays\n",
    "        Corrected peak positions\n",
    "    \"\"\"\n",
    "    if corrector is None:\n",
    "        return original_x, original_y\n",
    "    \n",
    "    try:\n",
    "        corrected_x, corrected_y = corrector.correct(original_x, original_y, scores)\n",
    "        return corrected_x, corrected_y\n",
    "    except Exception as e:\n",
    "        print(f\"  Warning: Correction failed, using original peaks. Error: {e}\")\n",
    "        return original_x, original_y\n",
    "\n",
    "# ===============================================================================\n",
    "# File Processing Function\n",
    "# ===============================================================================\n",
    "\n",
    "def process_single_file(file_info):\n",
    "    \"\"\"\n",
    "    Process a single file\n",
    "    \n",
    "    Parameters:\n",
    "    - file_info: tuple of (ifile, name, proc_name, weight, file_index, total_files)\n",
    "    \n",
    "    Returns:\n",
    "    - dict with processing results\n",
    "    \"\"\"\n",
    "    ifile, name, proc_name, weight, file_index, total_files = file_info\n",
    "    \n",
    "    try:\n",
    "        print(f\"[Worker {os.getpid()}] [{file_index}/{total_files}] Processing: {name}\")\n",
    "        \n",
    "        process_modellite = (proc_name in PROCESS_WITH_MODELLITE)\n",
    "        num_models = 4 if process_modellite else 3\n",
    "        \n",
    "        model_data = {}\n",
    "        model_peak_positions = {}\n",
    "        model_fit_info = {}\n",
    "        \n",
    "        for model_idx in range(num_models):\n",
    "            ensemble_model_name = ensemble_models[model_idx]\n",
    "            \n",
    "            pred_file = f\"{pred_folder}/{ensemble_model_name}/pred_{name}.root\"\n",
    "            dcbfit_file = f\"/data/bond/tyyang99/HH4b/dcb_results/FullNewMethodv2/{ensemble_model_name.split('/')[-1]}/pred_{name}_combined_fit_results.npz\"\n",
    "            \n",
    "            try:\n",
    "                pred_data_tmp = uproot.lazy(pred_file)\n",
    "                pred_data = pred_data_tmp[(pred_data_tmp['pass_selection']==1) & \n",
    "                                         (pred_data_tmp['pass_4j3b_selection']==1)]\n",
    "                \n",
    "                scores_ALLHH4b = np.zeros_like(pred_data['score_0'])\n",
    "                \n",
    "                for j in range(136):\n",
    "                    scores_ALLHH4b = scores_ALLHH4b + pred_data[f'score_{j}']\n",
    "                \n",
    "                pred_data['score_hh4bvsboth'] = scores_ALLHH4b / (\n",
    "                    scores_ALLHH4b + pred_data['score_136'] + pred_data['score_137'])\n",
    "                pred_data['score_hh4bvsqcd'] = scores_ALLHH4b / (\n",
    "                    scores_ALLHH4b + pred_data['score_136'])\n",
    "                \n",
    "                cut_value = 0\n",
    "                cut = (pred_data['score_hh4bvsboth'] > cut_value)\n",
    "                pred_data = pred_data[cut]\n",
    "                \n",
    "                fit_results = np.load(dcbfit_file)\n",
    "                \n",
    "                fit_success = fit_results['fit_success'][cut]\n",
    "                p1_amp = fit_results['p1_amp'][cut]\n",
    "                p2_amp = np.where(fit_results['p2_amp'][cut], \n",
    "                                 fit_results['p2_amp'][cut], 1e-9)\n",
    "                p1_x_mean = fit_results['p1_x_mean'][cut]\n",
    "                p1_y_mean = fit_results['p1_y_mean'][cut]\n",
    "                \n",
    "                swap_mask = p1_x_mean > p1_y_mean\n",
    "                original_peak_x = np.where(swap_mask, p1_y_mean, p1_x_mean)\n",
    "                original_peak_y = np.where(swap_mask, p1_x_mean, p1_y_mean)\n",
    "                \n",
    "                if model_idx == 0:\n",
    "                    model_name = 'Model0'\n",
    "                elif model_idx == 1:\n",
    "                    model_name = 'Model1'\n",
    "                elif model_idx == 2:\n",
    "                    model_name = 'Model2'\n",
    "                else:\n",
    "                    model_name = 'ModelLite'\n",
    "                \n",
    "                corrector = CORRECTORS[model_name]\n",
    "                scores_for_correction = ak.to_numpy(pred_data['score_hh4bvsboth'])\n",
    "                \n",
    "                final_peak_x, final_peak_y = apply_correction(\n",
    "                    original_peak_x, original_peak_y, scores_for_correction, corrector)\n",
    "                \n",
    "                amp_cut = (abs(p1_amp/p2_amp) > 0)\n",
    "                basic_fit_cut = (fit_success == 1) & amp_cut\n",
    "                \n",
    "                model_data[model_idx] = {\n",
    "                    'score_hh4bvsqcd': ak.to_numpy(pred_data['score_hh4bvsqcd']),\n",
    "                    'score_hh4bvsboth': scores_for_correction,\n",
    "                }\n",
    "                model_peak_positions[model_idx] = {\n",
    "                    'original_x': original_peak_x,\n",
    "                    'original_y': original_peak_y,\n",
    "                    'final_x': final_peak_x,\n",
    "                    'final_y': final_peak_y,\n",
    "                }\n",
    "                model_fit_info[model_idx] = {\n",
    "                    'fit_success': fit_success,\n",
    "                    'basic_fit_cut': basic_fit_cut,\n",
    "                    'p1_amp': p1_amp,\n",
    "                    'p2_amp': p2_amp\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[Worker {os.getpid()}] Error processing model {model_idx} for {name}: {e}\")\n",
    "                model_data[model_idx] = None\n",
    "                model_peak_positions[model_idx] = None\n",
    "                model_fit_info[model_idx] = None\n",
    "        \n",
    "        if any(model_data.get(i) is None for i in range(num_models)):\n",
    "            return {\n",
    "                'success': False,\n",
    "                'name': name,\n",
    "                'reason': 'Missing model data'\n",
    "            }\n",
    "        \n",
    "        lengths = [len(model_data[i]['score_hh4bvsqcd']) for i in range(num_models)]\n",
    "        if not all(length == lengths[0] for length in lengths):\n",
    "            return {\n",
    "                'success': False,\n",
    "                'name': name,\n",
    "                'reason': f'Model data lengths mismatch: {lengths}'\n",
    "            }\n",
    "        \n",
    "        n_events = lengths[0]\n",
    "        \n",
    "        if n_events == 0:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'name': name,\n",
    "                'reason': 'No events after basic selection'\n",
    "            }\n",
    "        \n",
    "        original_peaks_x = np.array([model_peak_positions[i]['original_x'] for i in range(3)])\n",
    "        original_peaks_y = np.array([model_peak_positions[i]['original_y'] for i in range(3)])\n",
    "        \n",
    "        final_peaks_x = np.array([model_peak_positions[i]['final_x'] for i in range(3)])\n",
    "        final_peaks_y = np.array([model_peak_positions[i]['final_y'] for i in range(3)])\n",
    "        \n",
    "        dist_01 = np.sqrt((original_peaks_x[0] - original_peaks_x[1])**2 + \n",
    "                         (original_peaks_y[0] - original_peaks_y[1])**2)\n",
    "        dist_02 = np.sqrt((original_peaks_x[0] - original_peaks_x[2])**2 + \n",
    "                         (original_peaks_y[0] - original_peaks_y[2])**2)\n",
    "        dist_12 = np.sqrt((original_peaks_x[1] - original_peaks_x[2])**2 + \n",
    "                         (original_peaks_y[1] - original_peaks_y[2])**2)\n",
    "        \n",
    "        all_distances = np.stack([dist_01, dist_02, dist_12], axis=0)\n",
    "        min_dist_idx = np.argmin(all_distances, axis=0)\n",
    "        \n",
    "        conditions = [\n",
    "            min_dist_idx == 0,\n",
    "            min_dist_idx == 1,\n",
    "            min_dist_idx == 2\n",
    "        ]\n",
    "        \n",
    "        choices_x_original = [\n",
    "            (original_peaks_x[0] + original_peaks_x[1]) / 2.0,\n",
    "            (original_peaks_x[0] + original_peaks_x[2]) / 2.0,\n",
    "            (original_peaks_x[1] + original_peaks_x[2]) / 2.0\n",
    "        ]\n",
    "        ensemble_original_peak_x = np.select(conditions, choices_x_original, default=125.0)\n",
    "        \n",
    "        choices_y_original = [\n",
    "            (original_peaks_y[0] + original_peaks_y[1]) / 2.0,\n",
    "            (original_peaks_y[0] + original_peaks_y[2]) / 2.0,\n",
    "            (original_peaks_y[1] + original_peaks_y[2]) / 2.0\n",
    "        ]\n",
    "        ensemble_original_peak_y = np.select(conditions, choices_y_original, default=125.0)\n",
    "        \n",
    "        ensemble_scores = (model_data[0]['score_hh4bvsboth'] + \n",
    "                          model_data[1]['score_hh4bvsboth'] + \n",
    "                          model_data[2]['score_hh4bvsboth']) / 3.0\n",
    "        \n",
    "        ensemble_corrector = CORRECTORS['Ensemble']\n",
    "        ensemble_final_peak_x, ensemble_final_peak_y = apply_correction(\n",
    "            ensemble_original_peak_x, ensemble_original_peak_y, ensemble_scores, ensemble_corrector)\n",
    "        \n",
    "        dist_01_final = np.sqrt((final_peaks_x[0] - final_peaks_x[1])**2 + \n",
    "                               (final_peaks_y[0] - final_peaks_y[1])**2)\n",
    "        dist_02_final = np.sqrt((final_peaks_x[0] - final_peaks_x[2])**2 + \n",
    "                               (final_peaks_y[0] - final_peaks_y[2])**2)\n",
    "        dist_12_final = np.sqrt((final_peaks_x[1] - final_peaks_x[2])**2 + \n",
    "                               (final_peaks_y[1] - final_peaks_y[2])**2)\n",
    "        \n",
    "        all_distances_final = np.stack([dist_01_final, dist_02_final, dist_12_final], axis=0)\n",
    "        min_dist_idx_final = np.argmin(all_distances_final, axis=0)\n",
    "        \n",
    "        conditions_final = [\n",
    "            min_dist_idx_final == 0,\n",
    "            min_dist_idx_final == 1,\n",
    "            min_dist_idx_final == 2\n",
    "        ]\n",
    "        \n",
    "        choices_x_final = [\n",
    "            (final_peaks_x[0] + final_peaks_x[1]) / 2.0,\n",
    "            (final_peaks_x[0] + final_peaks_x[2]) / 2.0,\n",
    "            (final_peaks_x[1] + final_peaks_x[2]) / 2.0\n",
    "        ]\n",
    "        ensemble_final_peak_v2_x = np.select(conditions_final, choices_x_final, default=125.0)\n",
    "        \n",
    "        choices_y_final = [\n",
    "            (final_peaks_y[0] + final_peaks_y[1]) / 2.0,\n",
    "            (final_peaks_y[0] + final_peaks_y[2]) / 2.0,\n",
    "            (final_peaks_y[1] + final_peaks_y[2]) / 2.0\n",
    "        ]\n",
    "        ensemble_final_peak_v2_y = np.select(conditions_final, choices_y_final, default=125.0)\n",
    "        \n",
    "        combined_basic_fit_cut = (\n",
    "            np.sum([model_fit_info[0]['basic_fit_cut'], \n",
    "                   model_fit_info[1]['basic_fit_cut'], \n",
    "                   model_fit_info[2]['basic_fit_cut']], axis=0) >= 3\n",
    "        )\n",
    "        \n",
    "        output_data = {\n",
    "            'ensemble_original_peak_x': ensemble_original_peak_x,\n",
    "            'ensemble_original_peak_y': ensemble_original_peak_y,\n",
    "            'ensemble_final_peak_x': ensemble_final_peak_x,\n",
    "            'ensemble_final_peak_y': ensemble_final_peak_y,\n",
    "            'ensemble_final_peak_v2_x': ensemble_final_peak_v2_x,\n",
    "            'ensemble_final_peak_v2_y': ensemble_final_peak_v2_y,\n",
    "            'min_dist_pair_idx_final': min_dist_idx_final.astype(np.int32),\n",
    "            \n",
    "            'model0_original_peak_x': original_peaks_x[0],\n",
    "            'model0_original_peak_y': original_peaks_y[0],\n",
    "            'model0_final_peak_x': final_peaks_x[0],\n",
    "            'model0_final_peak_y': final_peaks_y[0],\n",
    "            \n",
    "            'model1_original_peak_x': original_peaks_x[1],\n",
    "            'model1_original_peak_y': original_peaks_y[1],\n",
    "            'model1_final_peak_x': final_peaks_x[1],\n",
    "            'model1_final_peak_y': final_peaks_y[1],\n",
    "            \n",
    "            'model2_original_peak_x': original_peaks_x[2],\n",
    "            'model2_original_peak_y': original_peaks_y[2],\n",
    "            'model2_final_peak_x': final_peaks_x[2],\n",
    "            'model2_final_peak_y': final_peaks_y[2],\n",
    "            \n",
    "            'model0_score_hh4bvsqcd': model_data[0]['score_hh4bvsqcd'],\n",
    "            'model1_score_hh4bvsqcd': model_data[1]['score_hh4bvsqcd'],\n",
    "            'model2_score_hh4bvsqcd': model_data[2]['score_hh4bvsqcd'],\n",
    "            'model0_score_hh4bvsboth': model_data[0]['score_hh4bvsboth'],\n",
    "            'model1_score_hh4bvsboth': model_data[1]['score_hh4bvsboth'],\n",
    "            'model2_score_hh4bvsboth': model_data[2]['score_hh4bvsboth'],\n",
    "            \n",
    "            'model0_fit_success': model_fit_info[0]['fit_success'].astype(np.int32),\n",
    "            'model1_fit_success': model_fit_info[1]['fit_success'].astype(np.int32),\n",
    "            'model2_fit_success': model_fit_info[2]['fit_success'].astype(np.int32),\n",
    "            'model0_basic_fit_cut': model_fit_info[0]['basic_fit_cut'].astype(np.int32),\n",
    "            'model1_basic_fit_cut': model_fit_info[1]['basic_fit_cut'].astype(np.int32),\n",
    "            'model2_basic_fit_cut': model_fit_info[2]['basic_fit_cut'].astype(np.int32),\n",
    "            'model0_p1_amp': model_fit_info[0]['p1_amp'],\n",
    "            'model1_p1_amp': model_fit_info[1]['p1_amp'],\n",
    "            'model2_p1_amp': model_fit_info[2]['p1_amp'],\n",
    "            'model0_p2_amp': model_fit_info[0]['p2_amp'],\n",
    "            'model1_p2_amp': model_fit_info[1]['p2_amp'],\n",
    "            'model2_p2_amp': model_fit_info[2]['p2_amp'],\n",
    "            \n",
    "            'combined_basic_fit_cut': combined_basic_fit_cut.astype(np.int32),\n",
    "            'min_dist_pair_idx': min_dist_idx.astype(np.int32),\n",
    "            \n",
    "            'weight': np.ones(n_events) * weight\n",
    "        }\n",
    "        \n",
    "        if process_modellite:\n",
    "            output_data.update({\n",
    "                'modellite_original_peak_x': model_peak_positions[3]['original_x'],\n",
    "                'modellite_original_peak_y': model_peak_positions[3]['original_y'],\n",
    "                'modellite_final_peak_x': model_peak_positions[3]['final_x'],\n",
    "                'modellite_final_peak_y': model_peak_positions[3]['final_y'],\n",
    "                'modellite_score_hh4bvsqcd': model_data[3]['score_hh4bvsqcd'],\n",
    "                'modellite_score_hh4bvsboth': model_data[3]['score_hh4bvsboth'],\n",
    "                'modellite_fit_success': model_fit_info[3]['fit_success'].astype(np.int32),\n",
    "                'modellite_basic_fit_cut': model_fit_info[3]['basic_fit_cut'].astype(np.int32),\n",
    "                'modellite_p1_amp': model_fit_info[3]['p1_amp'],\n",
    "                'modellite_p2_amp': model_fit_info[3]['p2_amp'],\n",
    "            })\n",
    "        \n",
    "        output_proc_dir = os.path.join(output_base_dir, proc_name)\n",
    "        os.makedirs(output_proc_dir, exist_ok=True)\n",
    "        \n",
    "        output_file = os.path.join(output_proc_dir, f\"ensemble_method2_{name}.root\")\n",
    "        \n",
    "        with uproot.recreate(output_file) as f:\n",
    "            f[\"tree\"] = output_data\n",
    "        \n",
    "        n_saved = len(output_data['ensemble_original_peak_x'])\n",
    "        n_pass_combined = np.sum(combined_basic_fit_cut)\n",
    "        \n",
    "        modellite_status = \"with modellite\" if process_modellite else \"without modellite\"\n",
    "        print(f\"[Worker {os.getpid()}] Saved {n_saved} events ({n_pass_combined} pass combined_basic_fit_cut) \"\n",
    "              f\"to: {output_file} ({modellite_status})\")\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'name': name,\n",
    "            'proc_name': proc_name,\n",
    "            'n_events': n_events,\n",
    "            'n_saved': n_saved,\n",
    "            'n_pass_combined': n_pass_combined,\n",
    "            'output_file': output_file,\n",
    "            'has_modellite': process_modellite,\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[Worker {os.getpid()}] Error processing {name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'success': False,\n",
    "            'name': name,\n",
    "            'reason': str(e)\n",
    "        }\n",
    "\n",
    "# ===============================================================================\n",
    "# Main Processing Functions\n",
    "# ===============================================================================\n",
    "\n",
    "def collect_file_list():\n",
    "    \"\"\"\n",
    "    Collect all files to be processed\n",
    "    \n",
    "    Returns:\n",
    "    - List of file_info tuples\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    matching_folders = glob.glob(folder_pattern)\n",
    "    \n",
    "    for ifolder in matching_folders:\n",
    "        proc_name = ifolder.split(\"/\")[-1].split(\"_\")[0]\n",
    "        \n",
    "        if \"forInfer2\" in ifolder:\n",
    "            continue\n",
    "        \n",
    "        if proc_name not in process_list:\n",
    "            continue\n",
    "        \n",
    "        weight = weight_dict[proc_name]\n",
    "        matching_files = glob.glob(ifolder + \"/*\")\n",
    "        \n",
    "        if proc_name == \"QCD\" or proc_name == \"TTbar\":\n",
    "            ext_folder = ifolder.replace(\"forInfer\", \"forInfer2\")\n",
    "            matching_files += glob.glob(ext_folder + \"/*\")\n",
    "        \n",
    "        for ifile in matching_files:\n",
    "            if \"forInfer2\" in ifile:\n",
    "                name = proc_name + \"EXT_\" + ifile.replace(\".root\", \"\").split(\"_\")[-1]\n",
    "            else:\n",
    "                name = proc_name + \"_\" + ifile.replace(\".root\", \"\").split(\"_\")[-1]\n",
    "            \n",
    "            file_list.append((ifile, name, proc_name, weight))\n",
    "    \n",
    "    return file_list\n",
    "\n",
    "def process_ensemble_method2_parallel(n_workers=None):\n",
    "    \"\"\"\n",
    "    Process all files using ensemble method2 with parallel processing\n",
    "    \n",
    "    Parameters:\n",
    "    - n_workers: Number of parallel workers (default: use N_WORKERS)\n",
    "    \"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = N_WORKERS\n",
    "    \n",
    "    print(f\"Collecting file list...\")\n",
    "    file_list = collect_file_list()\n",
    "    total_files = len(file_list)\n",
    "    \n",
    "    if total_files == 0:\n",
    "        print(\"No files to process!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {total_files} files to process\")\n",
    "    print(f\"Using {n_workers} parallel workers\")\n",
    "    \n",
    "    file_list_with_index = [\n",
    "        (ifile, name, proc_name, weight, idx+1, total_files)\n",
    "        for idx, (ifile, name, proc_name, weight) in enumerate(file_list)\n",
    "    ]\n",
    "    \n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        results = pool.map(process_single_file, file_list_with_index)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Processing Summary\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    successful = [r for r in results if r['success']]\n",
    "    failed = [r for r in results if not r['success']]\n",
    "    \n",
    "    print(f\"Total files processed: {total_files}\")\n",
    "    print(f\"Successful: {len(successful)}\")\n",
    "    print(f\"Failed: {len(failed)}\")\n",
    "    \n",
    "    if successful:\n",
    "        total_saved = sum(r['n_saved'] for r in successful)\n",
    "        total_pass_combined = sum(r['n_pass_combined'] for r in successful)\n",
    "        n_with_modellite = sum(1 for r in successful if r.get('has_modellite', False))\n",
    "        n_without_modellite = len(successful) - n_with_modellite\n",
    "        \n",
    "        print(f\"\\nTotal events saved: {total_saved}\")\n",
    "        print(f\"Events passing combined_basic_fit_cut: {total_pass_combined} ({100*total_pass_combined/total_saved:.2f}%)\")\n",
    "        print(f\"Files with modellite: {n_with_modellite}\")\n",
    "        print(f\"Files without modellite: {n_without_modellite}\")\n",
    "        \n",
    "        proc_summary = {}\n",
    "        for r in successful:\n",
    "            proc = r['proc_name']\n",
    "            if proc not in proc_summary:\n",
    "                proc_summary[proc] = {'files': 0, 'events': 0, 'pass_combined': 0}\n",
    "            proc_summary[proc]['files'] += 1\n",
    "            proc_summary[proc]['events'] += r['n_saved']\n",
    "            proc_summary[proc]['pass_combined'] += r['n_pass_combined']\n",
    "        \n",
    "        print(\"\\nSummary by process:\")\n",
    "        for proc, stats in sorted(proc_summary.items()):\n",
    "            pass_rate = 100 * stats['pass_combined'] / stats['events'] if stats['events'] > 0 else 0\n",
    "            modellite_marker = \"✓\" if proc in PROCESS_WITH_MODELLITE else \"✗\"\n",
    "            print(f\"  {proc} [modellite:{modellite_marker}]: {stats['files']} files, {stats['events']} events, \"\n",
    "                  f\"{stats['pass_combined']} pass combined cut ({pass_rate:.2f}%)\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\nFailed files:\")\n",
    "        for r in failed:\n",
    "            print(f\"  {r['name']}: {r.get('reason', 'Unknown error')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"Starting Ensemble Method2 Ntuple Generation\")\n",
    "    print(\"with Diagonal Slice CDF Correction\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nOutput directory: {output_base_dir}\")\n",
    "    print(f\"Available CPU cores: {cpu_count()}\")\n",
    "    print(f\"Using {N_WORKERS} workers\")\n",
    "    print(f\"\\nCorrection method: Diagonal slice-based CDF matching with dynamic extension\")\n",
    "    print(f\"  - Source: Saved smooth 2D distributions\")\n",
    "    print(f\"  - Target: Tomography-corrected distributions\")\n",
    "    print(f\"  - Correction range (M): [70, 170] GeV\")\n",
    "    print(f\"  - Extended sampling range: [60, 180] GeV (linear from center)\")\n",
    "    print(f\"    * M = 120: sampling in [70, 170]\")\n",
    "    print(f\"    * M = 70:  sampling in [60, 170]\")\n",
    "    print(f\"    * M = 170: sampling in [70, 180]\")\n",
    "    print(f\"\\nModellite processing:\")\n",
    "    print(f\"  - Enabled for: {', '.join(PROCESS_WITH_MODELLITE)}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    process_ensemble_method2_parallel()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Processing completed!\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ada20-6a61-4713-b969-fe01c1b2beec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
